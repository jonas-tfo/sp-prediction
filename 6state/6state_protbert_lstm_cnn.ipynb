{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb673bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MODEL_NAME = \"Rostlab/prot_bert\" \n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "NUM_CLASSES = 6  # num classes for classification\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 0.001\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "DATA_PATH = \"/content/drive/MyDrive/PBLRost/data/reduced_30/\"\n",
    "DATA_PATH_FOLDS = \"/content/drive/MyDrive/PBLRost/data/reduced_30/5-fold/\"\n",
    "MODEL_SAVE_PATH_TEMP = \"/content/drive/MyDrive/PBLRost/models/6state_protbert_lstm_cnn_fold{}.pt\"\n",
    "MODEL_SAVE_PATH = \"/content/drive/MyDrive/PBLRost/models/6state_protbert_lstm_cnn.pt\"\n",
    "TRAIN_VAL_LOSSES_DATA_SAVE_PATH = \"/content/drive/MyDrive/PBLRost/data/reduced_30/outputs/\"\n",
    "TEST_CSV = os.path.join(DATA_PATH, \"reduced_30_signalP6_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc30469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "print(f\"Test records: {len(test_df)}\")\n",
    "\n",
    "# Display sample\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c475a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'S': 0, 'T': 1, 'L': 2, 'I': 3, 'M': 4, 'O': 5}\n",
    "\n",
    "print(\"\\nProcessing test data...\")\n",
    "test_df_filtered = test_df[~test_df[\"labels\"].str.contains(\"P\", na=False)]\n",
    "print(f\"Test records after filtering: {len(test_df_filtered)}\")\n",
    "\n",
    "test_df_filtered.describe()\n",
    "\n",
    "test_df_encoded = test_df_filtered.copy()\n",
    "test_df_encoded[\"label\"] = test_df_encoded[\"labels\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
    "test_df_encoded = test_df_encoded[test_df_encoded[\"label\"].map(len) > 0]\n",
    "test_seqs = test_df_encoded[\"sequence\"].tolist()\n",
    "test_label_seqs = test_df_encoded[\"label\"].tolist()\n",
    "\n",
    "print(f\"Test sequences: {len(test_seqs)}\")\n",
    "test_df_encoded.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d69cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
    "encoder.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d54f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPDataset(Dataset):\n",
    "    def __init__(self, sequences, label_seqs, label_map):\n",
    "        self.label_map = label_map\n",
    "        self.label_seqs = label_seqs\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        # insert spaces between amino acids\n",
    "        seq_processed = \" \".join(list(seq))\n",
    "        labels = self.label_seqs[idx]\n",
    "        # Tokenize the sequence (padding here might be redundant) \n",
    "        encoded = tokenizer(seq_processed, return_tensors=\"pt\",\n",
    "                            padding=\"max_length\", truncation=True, max_length=512)\n",
    "        input_ids = encoded['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Build a label tensor of the same length as input_ids. For tokens beyond original sequence length, assign -100 so that loss func ignores them\n",
    "        orig_length = len(seq)\n",
    "        token_labels = []\n",
    "        \n",
    "        for i in range(input_ids.size(0)):\n",
    "            if i == 0 or i > orig_length:  \n",
    "                token_labels.append(-100)  # ignore padding tokens\n",
    "            else:\n",
    "                # Use the already encoded label directly\n",
    "                token_labels.append(labels[i-1])\n",
    "        labels_tensor = torch.tensor(token_labels)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids, # tokenized and padded \n",
    "            'attention_mask': attention_mask, # differentiate between padding and non-padding tokens\n",
    "            'labels': labels_tensor # aligned label tensor\n",
    "        }\n",
    "\n",
    "# Test dataset (not yet needed)\n",
    "test_dataset = SPDataset(test_seqs, test_label_seqs, label_map)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nTest data prepared: {len(test_seqs)} sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43de5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchcrf import CRF\n",
    "\n",
    "class SPCNNClassifier(nn.Module):\n",
    "    def __init__(self, encoder_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_model  \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=hidden_size, out_channels=1024, kernel_size=8, dilation=2, padding=7)\n",
    "        # Normalize the conv output (should expect shape: (batch, 1024, seq_len))\n",
    "        self.bn_conv = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=1024, hidden_size=512, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        # 1 dense layer\n",
    "        self.classifier = nn.Linear(512 * 2, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \n",
    "        # encoded with bert\n",
    "        encoder_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = encoder_output.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Apply conv, then batch normalization and ReLU\n",
    "        x_conv = self.conv(hidden_states.transpose(1, 2))  # (batch, 1024, seq_len)\n",
    "        x_conv = self.bn_conv(x_conv)\n",
    "        x_conv = F.relu_(x_conv)                          # (batch, 1024, seq_len)\n",
    "\n",
    "        # Transpose CNN output for lstm: (batch, seq_len, features)\n",
    "        x_lstm_input = x_conv.transpose(1, 2)           # (batch, seq_len, 1024)\n",
    "\n",
    "        # Apply lstm\n",
    "        lstm_out, _ = self.lstm(x_lstm_input)            # (batch, seq_len, 1024)\n",
    "\n",
    "        # classifier to num_labels\n",
    "        x_linear = self.classifier(lstm_out)             # (batch, seq_len, num_labels)\n",
    "        logits = self.dropout(x_linear)                  # (batch, seq_len, num_labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            # Replace ignore-index (-100) with a valid label (0) (crf doesn't support -100)\n",
    "            mod_labels = labels.clone()\n",
    "            mod_labels[labels == -100] = 0\n",
    "            loss = -self.crf(logits, mod_labels, mask=attention_mask.bool(), reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            predictions = self.crf.decode(logits, mask=attention_mask.bool())\n",
    "            return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# load and prepare data for a specific fold\n",
    "def prepare_fold_data(fold_num):\n",
    "    \n",
    "    train_csv = os.path.join(DATA_PATH, f\"fold_{fold_num}_train.csv\")\n",
    "    val_csv = os.path.join(DATA_PATH, f\"fold_{fold_num}_val.csv\")\n",
    "    \n",
    "    print(f\"\\n=== Fold {fold_num} ===\")\n",
    "    print(f\"Loading training data from: {train_csv}\")\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    print(f\"Training records: {len(train_df)}\")\n",
    "    \n",
    "    print(f\"Loading validation data from: {val_csv}\")\n",
    "    val_df = pd.read_csv(val_csv)\n",
    "    print(f\"Validation records: {len(val_df)}\")\n",
    "    \n",
    "    # Filter data\n",
    "    train_df_filtered = train_df[~train_df[\"labels\"].str.contains(\"P\", na=False)]\n",
    "    val_df_filtered = val_df[~val_df[\"labels\"].str.contains(\"P\", na=False)]\n",
    "    print(f\"Training records after filtering: {len(train_df_filtered)}\")\n",
    "    print(f\"Validation records after filtering: {len(val_df_filtered)}\")\n",
    "    \n",
    "    # Encode labels\n",
    "    train_df_encoded = train_df_filtered.copy()\n",
    "    train_df_encoded[\"label\"] = train_df_encoded[\"labels\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
    "    train_df_encoded = train_df_encoded[train_df_encoded[\"label\"].map(len) > 0]\n",
    "    train_seqs = train_df_encoded[\"sequence\"].tolist()\n",
    "    train_label_seqs = train_df_encoded[\"label\"].tolist()\n",
    "    print(\"Encoded train data:\")\n",
    "    train_df_encoded.head()\n",
    "    \n",
    "    val_df_encoded = val_df_filtered.copy()\n",
    "    val_df_encoded[\"label\"] = val_df_encoded[\"labels\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
    "    val_df_encoded = val_df_encoded[val_df_encoded[\"label\"].map(len) > 0]\n",
    "    val_seqs = val_df_encoded[\"sequence\"].tolist()\n",
    "    val_label_seqs = val_df_encoded[\"label\"].tolist()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SPDataset(train_seqs, train_label_seqs, label_map)\n",
    "    val_dataset = SPDataset(val_seqs, val_label_seqs, label_map)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Fold {fold_num} prepared: {len(train_seqs)} train, {len(val_seqs)} val sequences\")\n",
    "    \n",
    "    return train_loader, val_loader, train_seqs, val_seqs\n",
    "\n",
    "print(\"Data preparation function ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d562c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence-level accuracy, skipping -100 (ignored) positions\n",
    "def sequence_level_accuracy(preds_flat, labels_flat, test_label_seqs):\n",
    "    # reconstruct the sequences from the flat predictions\n",
    "    seq_lengths = [len(seq) for seq in test_label_seqs]\n",
    "    preds_seq = []\n",
    "    labels_seq = []\n",
    "    idx = 0\n",
    "    for l in seq_lengths:\n",
    "        preds_seq.append(preds_flat[idx:idx+l])\n",
    "        labels_seq.append(labels_flat[idx:idx+l])\n",
    "        idx += l\n",
    "\n",
    "    # check if the valid predictions match the labels\n",
    "    correct = 0\n",
    "    for pred, label in zip(preds_seq, labels_seq):\n",
    "        is_valid = [l != -100 for l in label]\n",
    "        valid_preds = [p for p, valid in zip(pred, is_valid) if valid]\n",
    "        valid_labels = [l for l, valid in zip(label, is_valid) if valid]\n",
    "        if valid_preds == valid_labels:\n",
    "            correct += 1\n",
    "\n",
    "    total = len(seq_lengths)\n",
    "    return correct / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Store results for all folds\n",
    "fold_results = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'best_val_losses': [],\n",
    "    'fold_numbers': []\n",
    "}\n",
    "\n",
    "# Cross Validation Training Loop\n",
    "for fold in range(1, NUM_FOLDS + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting Fold {fold}/{NUM_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Prepare data for this fold\n",
    "    train_loader, val_loader, train_seqs, val_seqs = prepare_fold_data(fold)\n",
    "    \n",
    "    # Initialize fresh model for each fold\n",
    "    encoder_fold = BertModel.from_pretrained(MODEL_NAME)\n",
    "    encoder_fold.to(DEVICE)\n",
    "    model = SPCNNClassifier(encoder_fold, NUM_CLASSES).to(DEVICE)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": model.encoder.encoder.layer[-4:].parameters(), \"lr\": 5e-6},\n",
    "        {\"params\": model.conv.parameters(), \"lr\": 1e-3},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": 1e-3},\n",
    "        {\"params\": model.lstm.parameters(), \"lr\": 1e-3},\n",
    "        {\"params\": model.crf.parameters(), \"lr\": 1e-3},\n",
    "    ])\n",
    "    \n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Freeze encoder parameters initially\n",
    "    for param in model.encoder.encoder.layer[:-10].parameters():\n",
    "        param.requires_grad = False\n",
    "    model.encoder.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Track losses for this fold\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop for this fold\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"Fold {fold} - Epoch {epoch+1}/{EPOCHS} [Train]\", unit=\"batch\")\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch in pbar:\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                token_labels = batch['labels'].to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                loss = model(input_ids, attention_mask, token_labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(\"Error during training:\", e)\n",
    "                gc.collect()\n",
    "                continue\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Fold {fold} - Epoch {epoch+1}/{EPOCHS} [Val]\", unit=\"batch\"):\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                token_labels = batch['labels'].to(DEVICE)\n",
    "                \n",
    "                loss = model(input_ids, attention_mask, token_labels)\n",
    "                total_val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = total_val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Fold {fold} - Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model for fold (temporary)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model_path_temp = MODEL_SAVE_PATH_TEMP.format(fold)\n",
    "            torch.save(model.state_dict(), model_path_temp)\n",
    "            print(f\"  → Best model for fold {fold} saved to {model_path_temp}\")\n",
    "    \n",
    "    # Store results for fold\n",
    "    fold_results['train_losses'].append(train_losses)\n",
    "    fold_results['val_losses'].append(val_losses)\n",
    "    fold_results['best_val_losses'].append(best_val_loss)\n",
    "    fold_results['fold_numbers'].append(fold)\n",
    "    \n",
    "    print(f\"\\nBest validation loss for fold {fold}: {best_val_loss:.4f}\")\n",
    "    \n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"K-Fold Cross Validation Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print summary of all folds\n",
    "print(\"\\nSummary of all folds:\")\n",
    "for i, best_loss in enumerate(fold_results['best_val_losses'], 1):\n",
    "    print(f\"Fold {i}: Best Validation Loss = {best_loss:.4f}\")\n",
    "\n",
    "avg_best_val_loss = sum(fold_results['best_val_losses']) / NUM_FOLDS\n",
    "print(f\"\\nAverage Best Validation Loss across all folds: {avg_best_val_loss:.4f}\")\n",
    "\n",
    "# Find the best fold\n",
    "best_fold_idx = fold_results['best_val_losses'].index(min(fold_results['best_val_losses']))\n",
    "best_fold_num = fold_results['fold_numbers'][best_fold_idx]\n",
    "best_fold_loss = fold_results['best_val_losses'][best_fold_idx]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Best performing fold: Fold {best_fold_num}\")\n",
    "print(f\"Best validation loss: {best_fold_loss:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Copy best model to final MODEL_SAVE_PATH\n",
    "import shutil\n",
    "best_model_path = MODEL_SAVE_PATH_TEMP.format(best_fold_num)\n",
    "shutil.copy(best_model_path, MODEL_SAVE_PATH)\n",
    "print(f\"\\nBest model (Fold {best_fold_num}) saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Optionally, clean up temporary fold models\n",
    "print(\"\\nTemporary fold models:\")\n",
    "for i in range(1, NUM_FOLDS + 1):\n",
    "    temp_path = MODEL_SAVE_PATH_TEMP.format(i)\n",
    "    if os.path.exists(temp_path):\n",
    "        print(f\"  - {temp_path}\")\n",
    "\n",
    "# Save training and validation losses to pickle file\n",
    "os.makedirs(TRAIN_VAL_LOSSES_DATA_SAVE_PATH, exist_ok=True)\n",
    "losses_pickle_path = os.path.join(TRAIN_VAL_LOSSES_DATA_SAVE_PATH, \"train_val_losses.pkl\")\n",
    "\n",
    "losses_data = {\n",
    "    'fold_numbers': fold_results['fold_numbers'],\n",
    "    'train_losses': fold_results['train_losses'],\n",
    "    'val_losses': fold_results['val_losses'],\n",
    "    'best_val_losses': fold_results['best_val_losses'],\n",
    "    'best_fold_num': best_fold_num,\n",
    "    'best_fold_loss': best_fold_loss,\n",
    "    'avg_best_val_loss': avg_best_val_loss,\n",
    "    'epochs': EPOCHS,\n",
    "    'num_folds': NUM_FOLDS\n",
    "}\n",
    "\n",
    "with open(losses_pickle_path, 'wb') as f:\n",
    "    pickle.dump(losses_data, f)\n",
    "\n",
    "print(f\"\\nTraining and validation losses saved to: {losses_pickle_path}\")\n",
    "print(f\"Data structure:\")\n",
    "print(f\"  - fold_numbers: list of fold IDs\")\n",
    "print(f\"  - train_losses: list of training losses per fold (each fold has {EPOCHS} epochs)\")\n",
    "print(f\"  - val_losses: list of validation losses per fold (each fold has {EPOCHS} epochs)\")\n",
    "print(f\"  - best_val_losses: list of best validation loss for each fold\")\n",
    "print(f\"  - best_fold_num: {best_fold_num}\")\n",
    "print(f\"  - best_fold_loss: {best_fold_loss:.4f}\")\n",
    "print(f\"  - avg_best_val_loss: {avg_best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ec91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n best model saved at: {MODEL_SAVE_PATH}, from fold {best_fold_num}, validation loss: {best_fold_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c38879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score, matthews_corrcoef, accuracy_score\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Evaluating Best Model (Fold {best_fold_num})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the best model\n",
    "encoder_eval = BertModel.from_pretrained(MODEL_NAME)\n",
    "encoder_eval.to(DEVICE)\n",
    "model = SPCNNClassifier(encoder_eval, NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "print(f\"\\nModel loaded from {MODEL_SAVE_PATH}\")\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        # Compute loss using CRF\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Decode predictions using CRF\n",
    "        predictions = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Collect valid tokens\n",
    "        for pred_seq, label_seq, mask in zip(predictions, labels, attention_mask):\n",
    "            for pred, true, is_valid in zip(pred_seq, label_seq, mask):\n",
    "                if true.item() != -100 and is_valid.item() == 1:\n",
    "                    all_preds.append(pred)\n",
    "                    all_labels.append(true.item())\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test Set Results\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(label_map.keys())))\n",
    "\n",
    "f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "token_acc = accuracy_score(all_labels, all_preds)\n",
    "seq_acc = sequence_level_accuracy(all_preds, all_labels, test_label_seqs)\n",
    "avg_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"\\nMetrics Summary:\")\n",
    "print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
    "print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "print(f\"Token-level Accuracy: {token_acc:.4f}\")\n",
    "print(f\"Sequence Level Accuracy: {seq_acc:.4f}\")\n",
    "print(f\"Average test loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=list(label_map.values()))\n",
    "cm_relative = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_relative, display_labels=list(label_map.keys()))\n",
    "disp.plot(cmap=\"OrRd\", xticks_rotation=45)\n",
    "plt.title(f\"Confusion Matrix - Best Model (Fold {best_fold_num})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
