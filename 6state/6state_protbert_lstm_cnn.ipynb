{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb673bb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb673bb6",
        "outputId": "b1bc6927-e94f-4331-eb18-d13c80004075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Project base directory set to: /Users/jonas/Desktop/Uni/PBL/sp-prediction\n",
            "Data path set to: /Users/jonas/Desktop/Uni/PBL/sp-prediction/data/aufgabe3\n",
            "Model save path set to: /Users/jonas/Desktop/Uni/PBL/sp-prediction/models/6state_protbert_lstm_cnn.pt\n"
          ]
        }
      ],
      "source": [
        "import tqdm as notebook_tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_NAME = \"Rostlab/prot_bert\"\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "NUM_CLASSES = 6  # num classes for classification\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "NUM_FOLDS = 5\n",
        "\n",
        "notebook_dir = Path.cwd()\n",
        "BASE_DIR = notebook_dir.parent\n",
        "\n",
        "DATA_PATH = BASE_DIR / \"data\" / \"aufgabe3\"\n",
        "DATA_PATH_FOLDS = DATA_PATH / \"5-fold\"\n",
        "MODEL_SAVE_PATH_TEMP = str(BASE_DIR / \"models\" / \"6state_protbert_lstm_cnn_fold{}.pt\")\n",
        "MODEL_SAVE_PATH = BASE_DIR / \"models\" / \"6state_protbert_lstm_cnn.pt\"\n",
        "TRAIN_VAL_LOSSES_DATA_SAVE_PATH = DATA_PATH / \"outputs\"\n",
        "TEST_CSV = DATA_PATH / \"reduced_30_signalP6_test.csv\"\n",
        "\n",
        "(BASE_DIR / \"models\").mkdir(exist_ok=True)\n",
        "(DATA_PATH / \"outputs\").mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Project base directory set to: {BASE_DIR}\")\n",
        "print(f\"Data path set to: {DATA_PATH}\")\n",
        "print(f\"Model save path set to: {MODEL_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc30469",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "abc30469",
        "outputId": "7bb406f1-378a-477e-91df-b7bc57d561b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading test data...\n",
            "Test records: 3419\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "uniprot_id",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "kingdom",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "type",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "sequence",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "labels",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "length",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "has_sp",
                  "rawType": "bool",
                  "type": "boolean"
                }
              ],
              "ref": "7a095bf0-8886-4b12-aa01-ad8ea0b5d169",
              "rows": [
                [
                  "0",
                  "B2VGX7",
                  "NEGATIVE",
                  "TAT",
                  "MKAVNPLTENDVTPESLFNARRRTVLKMLGMSAAALSLPGAARADLLSWFKGGDRPRAASGRPLDFSQPQ",
                  "TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTOOOOOOOOOOOOOOOOOOOOOOOOOO",
                  "70",
                  "True"
                ],
                [
                  "1",
                  "Q9DEN4",
                  "EUKARYA",
                  "NO_SP",
                  "MTLSGSGSASDMSGQTVLSADDADIDVVGEGDEALDKDSECESTAGHTDEVGELGGKEIPRSPSGSGTEA",
                  "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII",
                  "70",
                  "False"
                ],
                [
                  "2",
                  "A9QM74",
                  "EUKARYA",
                  "NO_SP",
                  "MPTLDAPEERRRKFKYRGKDVSLRRQQRMAVSLELRKAKKDEQTLKRRNITSFCPDTPSEKTAKGVAVSL",
                  "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII",
                  "70",
                  "False"
                ],
                [
                  "3",
                  "Q8K3I4",
                  "EUKARYA",
                  "NO_SP",
                  "MGRKLDLSGLTDDETEHVLQVVQRDFNLRKKEEDRLSEMKQRLAEENSKCSILSKHQKFVERCCMRCCSP",
                  "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII",
                  "70",
                  "False"
                ],
                [
                  "4",
                  "Q920M2",
                  "EUKARYA",
                  "NO_SP",
                  "MEVPELGPGLVERLEQLATCPLCGGPFEDPVLLACEHSFCRSCLARCWGSPAAPGSEEATPSCPCCGQPC",
                  "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII",
                  "70",
                  "False"
                ]
              ],
              "shape": {
                "columns": 7,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniprot_id</th>\n",
              "      <th>kingdom</th>\n",
              "      <th>type</th>\n",
              "      <th>sequence</th>\n",
              "      <th>labels</th>\n",
              "      <th>length</th>\n",
              "      <th>has_sp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B2VGX7</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>TAT</td>\n",
              "      <td>MKAVNPLTENDVTPESLFNARRRTVLKMLGMSAAALSLPGAARADL...</td>\n",
              "      <td>TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTOO...</td>\n",
              "      <td>70</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q9DEN4</td>\n",
              "      <td>EUKARYA</td>\n",
              "      <td>NO_SP</td>\n",
              "      <td>MTLSGSGSASDMSGQTVLSADDADIDVVGEGDEALDKDSECESTAG...</td>\n",
              "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...</td>\n",
              "      <td>70</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A9QM74</td>\n",
              "      <td>EUKARYA</td>\n",
              "      <td>NO_SP</td>\n",
              "      <td>MPTLDAPEERRRKFKYRGKDVSLRRQQRMAVSLELRKAKKDEQTLK...</td>\n",
              "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...</td>\n",
              "      <td>70</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q8K3I4</td>\n",
              "      <td>EUKARYA</td>\n",
              "      <td>NO_SP</td>\n",
              "      <td>MGRKLDLSGLTDDETEHVLQVVQRDFNLRKKEEDRLSEMKQRLAEE...</td>\n",
              "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...</td>\n",
              "      <td>70</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q920M2</td>\n",
              "      <td>EUKARYA</td>\n",
              "      <td>NO_SP</td>\n",
              "      <td>MEVPELGPGLVERLEQLATCPLCGGPFEDPVLLACEHSFCRSCLAR...</td>\n",
              "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...</td>\n",
              "      <td>70</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  uniprot_id   kingdom   type  \\\n",
              "0     B2VGX7  NEGATIVE    TAT   \n",
              "1     Q9DEN4   EUKARYA  NO_SP   \n",
              "2     A9QM74   EUKARYA  NO_SP   \n",
              "3     Q8K3I4   EUKARYA  NO_SP   \n",
              "4     Q920M2   EUKARYA  NO_SP   \n",
              "\n",
              "                                            sequence  \\\n",
              "0  MKAVNPLTENDVTPESLFNARRRTVLKMLGMSAAALSLPGAARADL...   \n",
              "1  MTLSGSGSASDMSGQTVLSADDADIDVVGEGDEALDKDSECESTAG...   \n",
              "2  MPTLDAPEERRRKFKYRGKDVSLRRQQRMAVSLELRKAKKDEQTLK...   \n",
              "3  MGRKLDLSGLTDDETEHVLQVVQRDFNLRKKEEDRLSEMKQRLAEE...   \n",
              "4  MEVPELGPGLVERLEQLATCPLCGGPFEDPVLLACEHSFCRSCLAR...   \n",
              "\n",
              "                                              labels  length  has_sp  \n",
              "0  TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTOO...      70    True  \n",
              "1  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...      70   False  \n",
              "2  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...      70   False  \n",
              "3  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...      70   False  \n",
              "4  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...      70   False  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "print(\"Loading test data...\")\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "print(f\"Test records: {len(test_df)}\")\n",
        "\n",
        "test_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06c475a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "06c475a3",
        "outputId": "4b203a3f-d448-4154-edb3-37c5bc7a314e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing test data...\n",
            "Test records after filtering: 3412\n",
            "Test sequences: 3412\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "length",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "9e52bcb3-d737-4ed4-ae92-b7d651a6cf19",
              "rows": [
                [
                  "count",
                  "3412.0"
                ],
                [
                  "mean",
                  "70.0"
                ],
                [
                  "std",
                  "0.0"
                ],
                [
                  "min",
                  "70.0"
                ],
                [
                  "25%",
                  "70.0"
                ],
                [
                  "50%",
                  "70.0"
                ],
                [
                  "75%",
                  "70.0"
                ],
                [
                  "max",
                  "70.0"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 8
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3412.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       length\n",
              "count  3412.0\n",
              "mean     70.0\n",
              "std       0.0\n",
              "min      70.0\n",
              "25%      70.0\n",
              "50%      70.0\n",
              "75%      70.0\n",
              "max      70.0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_map = {'S': 0, 'T': 1, 'L': 2, 'I': 3, 'M': 4, 'O': 5}\n",
        "\n",
        "print(\"\\nProcessing test data...\")\n",
        "test_df_filtered = test_df[~test_df[\"labels\"].str.contains(\"P\", na=False)]\n",
        "print(f\"Test records after filtering: {len(test_df_filtered)}\")\n",
        "\n",
        "test_df_filtered.describe()\n",
        "\n",
        "test_df_encoded = test_df_filtered.copy()\n",
        "test_df_encoded[\"label\"] = test_df_encoded[\"labels\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
        "test_df_encoded = test_df_encoded[test_df_encoded[\"label\"].map(len) > 0]\n",
        "test_seqs = test_df_encoded[\"sequence\"].tolist()\n",
        "test_label_seqs = test_df_encoded[\"label\"].tolist()\n",
        "\n",
        "print(f\"Test sequences: {len(test_seqs)}\")\n",
        "test_df_encoded.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d69cb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86d69cb6",
        "outputId": "7f6466ec-5b91-4dbb-bc8d-fd313d673856"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30, 1024, padding_idx=0)\n",
              "    (position_embeddings): Embedding(40000, 1024)\n",
              "    (token_type_embeddings): Embedding(2, 1024)\n",
              "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-29): 30 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
        "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
        "encoder.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d54f7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63d54f7c",
        "outputId": "dbfe5bf7-39ab-4109-b455-e76866a9393d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test data prepared: 3412 sequences\n"
          ]
        }
      ],
      "source": [
        "class SPDataset(Dataset):\n",
        "    def __init__(self, sequences, label_seqs, label_map):\n",
        "        self.label_map = label_map\n",
        "        self.label_seqs = label_seqs\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        seq_processed = \" \".join(list(seq))\n",
        "        labels = self.label_seqs[idx]\n",
        "\n",
        "        encoded = tokenizer(seq_processed, return_tensors=\"pt\",\n",
        "                            padding=\"max_length\", truncation=True, max_length=512)\n",
        "        input_ids = encoded['input_ids'].squeeze(0)\n",
        "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Build label tensor accounting for [CLS], sequence, [SEP], [PAD]\n",
        "        token_labels = [-100]  # [CLS]\n",
        "\n",
        "        for label in labels:\n",
        "            token_labels.append(label)\n",
        "\n",
        "        token_labels.append(-100)  # [SEP]\n",
        "\n",
        "        # Pad to max_length\n",
        "        while len(token_labels) < input_ids.size(0):\n",
        "            token_labels.append(-100)\n",
        "\n",
        "        # Truncate if needed\n",
        "        token_labels = token_labels[:input_ids.size(0)]\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(token_labels)\n",
        "        }\n",
        "\n",
        "# Test dataset (not yet needed)\n",
        "test_dataset = SPDataset(test_seqs, test_label_seqs, label_map)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"\\nTest data prepared: {len(test_seqs)} sequences\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43de5ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e43de5ab",
        "outputId": "a2877a53-a7e2-4514-dc08-d1248a8e6890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-crf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchcrf import CRF\n",
        "\n",
        "class SPCNNClassifier(nn.Module):\n",
        "    def __init__(self, encoder_model, num_labels):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_model\n",
        "        self.dropout = nn.Dropout(0.35)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels=hidden_size, out_channels=1024, kernel_size=5, padding=2)\n",
        "        # Normalize the conv output (should expect shape: (batch, 1024, seq_len))\n",
        "        self.bn_conv = nn.BatchNorm1d(1024)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=1024, hidden_size=512, num_layers=2, bidirectional=True, batch_first=True)\n",
        "        # 1 dense layer\n",
        "        self.classifier = nn.Linear(512 * 2, num_labels)\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "\n",
        "        # encoded with bert\n",
        "        encoder_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = encoder_output.last_hidden_state  # (batch, seq_len, hidden_size)\n",
        "\n",
        "        # Apply conv, then batch normalization and ReLU\n",
        "        x_conv = self.conv(hidden_states.transpose(1, 2))  # (batch, 1024, seq_len)\n",
        "        x_conv = self.bn_conv(x_conv)\n",
        "        x_conv = F.relu_(x_conv)                          # (batch, 1024, seq_len)\n",
        "\n",
        "        # Transpose CNN output for lstm: (batch, seq_len, features)\n",
        "        x_lstm_input = x_conv.transpose(1, 2)           # (batch, seq_len, 1024)\n",
        "\n",
        "        # Apply lstm\n",
        "        lstm_out, _ = self.lstm(x_lstm_input)            # (batch, seq_len, 1024)\n",
        "\n",
        "        # classifier to num_labels\n",
        "        x_linear = self.classifier(lstm_out)             # (batch, seq_len, num_labels)\n",
        "        logits = self.dropout(x_linear)                  # (batch, seq_len, num_labels)\n",
        "\n",
        "        if labels is not None:\n",
        "            # Replace ignore-index (-100) with a valid label (0) (crf doesn't support -100)\n",
        "            mod_labels = labels.clone()\n",
        "            mod_labels[labels == -100] = 0\n",
        "            loss = -self.crf(logits, mod_labels, mask=attention_mask.bool(), reduction='mean')\n",
        "            return loss\n",
        "        else:\n",
        "            predictions = self.crf.decode(logits, mask=attention_mask.bool())\n",
        "            return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e0d88fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e0d88fd",
        "outputId": "fa07159c-2513-4a69-d93f-b462840039c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data preparation function ready\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# load and prepare data for a specific fold\n",
        "def prepare_fold_data(fold_num):\n",
        "\n",
        "    train_csv = os.path.join(DATA_PATH_FOLDS, f\"fold_{fold_num}_train.csv\")\n",
        "    val_csv = os.path.join(DATA_PATH_FOLDS, f\"fold_{fold_num}_val.csv\")\n",
        "\n",
        "    print(f\"\\n=== Fold {fold_num} ===\")\n",
        "    print(f\"Loading training data from: {train_csv}\")\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    print(f\"Training records: {len(train_df)}\")\n",
        "\n",
        "    print(f\"Loading validation data from: {val_csv}\")\n",
        "    val_df = pd.read_csv(val_csv)\n",
        "    print(f\"Validation records: {len(val_df)}\")\n",
        "\n",
        "    # Filter data\n",
        "    train_df_filtered = train_df[~train_df[\"labels\"].str.contains(\"P\", na=False)]\n",
        "    val_df_filtered = val_df[~val_df[\"labels\"].str.contains(\"P\", na=False)]\n",
        "    print(f\"Training records after filtering: {len(train_df_filtered)}\")\n",
        "    print(f\"Validation records after filtering: {len(val_df_filtered)}\")\n",
        "\n",
        "    # Encode labels\n",
        "    train_df_encoded = train_df_filtered.copy()\n",
        "    train_df_encoded[\"label\"] = train_df_encoded[\"labels\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
        "    train_df_encoded = train_df_encoded[train_df_encoded[\"label\"].map(len) > 0]\n",
        "    train_seqs = train_df_encoded[\"sequence\"].tolist()\n",
        "    train_label_seqs = train_df_encoded[\"label\"].tolist()\n",
        "    print(\"Encoded train data:\")\n",
        "    train_df_encoded.head()\n",
        "\n",
        "    val_df_encoded = val_df_filtered.copy()\n",
        "    val_df_encoded[\"label\"] = val_df_encoded[\"labels\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
        "    val_df_encoded = val_df_encoded[val_df_encoded[\"label\"].map(len) > 0]\n",
        "    val_seqs = val_df_encoded[\"sequence\"].tolist()\n",
        "    val_label_seqs = val_df_encoded[\"label\"].tolist()\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = SPDataset(train_seqs, train_label_seqs, label_map)\n",
        "    val_dataset = SPDataset(val_seqs, val_label_seqs, label_map)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(f\"Fold {fold_num} prepared: {len(train_seqs)} train, {len(val_seqs)} val sequences\")\n",
        "\n",
        "    return train_loader, val_loader, train_seqs, val_seqs\n",
        "\n",
        "print(\"Data preparation function ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d562c26",
      "metadata": {
        "id": "4d562c26"
      },
      "outputs": [],
      "source": [
        "# sequence-level accuracy, skipping -100 (ignored) positions\n",
        "def sequence_level_accuracy(preds_flat, labels_flat, test_label_seqs):\n",
        "    # reconstruct the sequences from the flat predictions\n",
        "    seq_lengths = [len(seq) for seq in test_label_seqs]\n",
        "    preds_seq = []\n",
        "    labels_seq = []\n",
        "    idx = 0\n",
        "    for l in seq_lengths:\n",
        "        preds_seq.append(preds_flat[idx:idx+l])\n",
        "        labels_seq.append(labels_flat[idx:idx+l])\n",
        "        idx += l\n",
        "\n",
        "    # check if the valid predictions match the labels\n",
        "    correct = 0\n",
        "    for pred, label in zip(preds_seq, labels_seq):\n",
        "        is_valid = [l != -100 for l in label]\n",
        "        valid_preds = [p for p, valid in zip(pred, is_valid) if valid]\n",
        "        valid_labels = [l for l, valid in zip(label, is_valid) if valid]\n",
        "        if valid_preds == valid_labels:\n",
        "            correct += 1\n",
        "\n",
        "    total = len(seq_lengths)\n",
        "    return correct / total if total > 0 else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d3bc313",
      "metadata": {
        "id": "9d3bc313"
      },
      "outputs": [],
      "source": [
        "\n",
        "def encoder_unfreeze(model, epoch):\n",
        "    if epoch < 2:\n",
        "        # Freeze encoder completely\n",
        "        for param in model.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        trainable = 0\n",
        "\n",
        "    elif epoch < 4:\n",
        "        # Unfreeze last 6 layers\n",
        "        for param in model.encoder.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "        for layer in model.encoder.encoder.layer[:-6]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "        for layer in model.encoder.encoder.layer[-6:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "        trainable = 6\n",
        "\n",
        "    else:\n",
        "        # Unfreeze all (but use very low LR)\n",
        "        for param in model.encoder.parameters():\n",
        "            param.requires_grad = True\n",
        "        trainable = len(model.encoder.encoder.layer)\n",
        "\n",
        "    print(f\"Epoch {epoch}: {trainable}/{len(model.encoder.encoder.layer)} encoder layers trainable\")\n",
        "    return trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d159eaca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d159eaca",
        "outputId": "33eceac2-0a5c-48f0-8037-65e0c73ec41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting Fold 1/5\n",
            "============================================================\n",
            "\n",
            "=== Fold 1 ===\n",
            "Loading training data from: /Users/jonas/Desktop/Uni/PBL/sp-prediction/data/aufgabe3/5-fold/fold_1_train.csv\n",
            "Training records: 10940\n",
            "Loading validation data from: /Users/jonas/Desktop/Uni/PBL/sp-prediction/data/aufgabe3/5-fold/fold_1_val.csv\n",
            "Validation records: 2735\n",
            "Training records after filtering: 10916\n",
            "Validation records after filtering: 2732\n",
            "Encoded train data:\n",
            "Fold 1 prepared: 10916 train, 2732 val sequences\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 0/30 encoder layers trainable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 1 - Epoch 1/10 [Train]:   3%|▎         | 10/342 [01:04<32:29,  5.87s/batch, loss=68.5]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.amp import autocast, GradScaler\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# Store results for all folds\n",
        "fold_results = {\n",
        "    'train_losses': [],\n",
        "    'val_losses': [],\n",
        "    'best_val_losses': [],\n",
        "    'fold_numbers': []\n",
        "}\n",
        "\n",
        "# Cross Validation Training Loop\n",
        "for fold in range(1, NUM_FOLDS + 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Starting Fold {fold}/{NUM_FOLDS}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Prepare data for this fold\n",
        "    train_loader, val_loader, train_seqs, val_seqs = prepare_fold_data(fold)\n",
        "\n",
        "    # Initialize fresh model for each fold\n",
        "    encoder_fold = BertModel.from_pretrained(MODEL_NAME)\n",
        "    encoder_fold.to(DEVICE)\n",
        "    model = SPCNNClassifier(encoder_fold, NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Track losses for this fold\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Training loop for this fold\n",
        "    for epoch in range(EPOCHS):\n",
        "        trainable_layers = encoder_unfreeze(model, epoch)\n",
        "\n",
        "        # Create optimizer based on current trainable parameters\n",
        "        if trainable_layers == 0:\n",
        "            # train head\n",
        "            optimizer = torch.optim.AdamW([\n",
        "                {\"params\": model.conv.parameters(), \"lr\": 1e-3},\n",
        "                {\"params\": model.classifier.parameters(), \"lr\": 1e-3},\n",
        "                {\"params\": model.lstm.parameters(), \"lr\": 1e-3},\n",
        "                {\"params\": model.crf.parameters(), \"lr\": 1e-3},\n",
        "            ])\n",
        "        elif trainable_layers == 6:\n",
        "            # train head + top layers\n",
        "            optimizer = torch.optim.AdamW([\n",
        "                {\"params\": model.encoder.encoder.layer[-6:].parameters(), \"lr\": 1e-5},\n",
        "                {\"params\": model.conv.parameters(), \"lr\": 5e-4},\n",
        "                {\"params\": model.classifier.parameters(), \"lr\": 5e-4},\n",
        "                {\"params\": model.lstm.parameters(), \"lr\": 5e-4},\n",
        "                {\"params\": model.crf.parameters(), \"lr\": 5e-4},\n",
        "            ])\n",
        "        else:\n",
        "            # Train all layers\n",
        "            optimizer = torch.optim.AdamW([\n",
        "                {\"params\": model.encoder.parameters(), \"lr\": 5e-6},\n",
        "                {\"params\": model.conv.parameters(), \"lr\": 1e-4},\n",
        "                {\"params\": model.classifier.parameters(), \"lr\": 1e-4},\n",
        "                {\"params\": model.lstm.parameters(), \"lr\": 1e-4},\n",
        "                {\"params\": model.crf.parameters(), \"lr\": 1e-4},\n",
        "            ])\n",
        "\n",
        "        # create scheduler for epoch\n",
        "        total_steps = len(train_loader)\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=int(0.1 * total_steps),\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Fold {fold} - Epoch {epoch+1}/{EPOCHS} [Train]\", unit=\"batch\")\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in pbar:\n",
        "            try:\n",
        "                input_ids = batch['input_ids'].to(DEVICE)\n",
        "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "                token_labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                loss = model(input_ids, attention_mask, token_labels)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(\"Error during training:\", e)\n",
        "                gc.collect()\n",
        "                continue\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Fold {fold} - Epoch {epoch+1}/{EPOCHS} [Val]\", unit=\"batch\"):\n",
        "                input_ids = batch['input_ids'].to(DEVICE)\n",
        "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "                token_labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "                loss = model(input_ids, attention_mask, token_labels)\n",
        "                total_val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val_loss = total_val_loss / val_batches if val_batches > 0 else 0\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f\"Fold {fold} - Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save best model for fold (temporary)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            model_path_temp = MODEL_SAVE_PATH_TEMP.format(fold)\n",
        "            torch.save(model.state_dict(), model_path_temp)\n",
        "            print(f\"  → Best model for fold {fold} saved to {model_path_temp}\")\n",
        "\n",
        "    # Store results for fold\n",
        "    fold_results['train_losses'].append(train_losses)\n",
        "    fold_results['val_losses'].append(val_losses)\n",
        "    fold_results['best_val_losses'].append(best_val_loss)\n",
        "    fold_results['fold_numbers'].append(fold)\n",
        "\n",
        "    print(f\"\\nBest validation loss for fold {fold}: {best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"K-Fold Cross Validation Complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nSummary of all folds:\")\n",
        "for i, best_loss in enumerate(fold_results['best_val_losses'], 1):\n",
        "    print(f\"Fold {i}: Best Validation Loss = {best_loss:.4f}\")\n",
        "\n",
        "avg_best_val_loss = sum(fold_results['best_val_losses']) / NUM_FOLDS\n",
        "print(f\"\\nAverage Best Validation Loss across all folds: {avg_best_val_loss:.4f}\")\n",
        "\n",
        "# Find best fold\n",
        "best_fold_idx = fold_results['best_val_losses'].index(min(fold_results['best_val_losses']))\n",
        "best_fold_num = fold_results['fold_numbers'][best_fold_idx]\n",
        "best_fold_loss = fold_results['best_val_losses'][best_fold_idx]\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Best performing fold: Fold {best_fold_num}\")\n",
        "print(f\"Best validation loss: {best_fold_loss:.4f}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# best model saved to MODEL_SAVE_PATH\n",
        "import shutil\n",
        "best_model_path = MODEL_SAVE_PATH_TEMP.format(best_fold_num)\n",
        "shutil.copy(best_model_path, MODEL_SAVE_PATH)\n",
        "print(f\"\\nBest model (Fold {best_fold_num}) saved to: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "# clean up of temporary fold models\n",
        "print(\"\\nTemporary fold models:\")\n",
        "for i in range(1, NUM_FOLDS + 1):\n",
        "    temp_path = MODEL_SAVE_PATH_TEMP.format(i)\n",
        "    if os.path.exists(temp_path):\n",
        "        print(f\"  - {temp_path}\")\n",
        "\n",
        "os.makedirs(TRAIN_VAL_LOSSES_DATA_SAVE_PATH, exist_ok=True)\n",
        "losses_pickle_path = os.path.join(TRAIN_VAL_LOSSES_DATA_SAVE_PATH, \"train_val_losses.pkl\")\n",
        "\n",
        "losses_data = {\n",
        "    'fold_numbers': fold_results['fold_numbers'],\n",
        "    'train_losses': fold_results['train_losses'],\n",
        "    'val_losses': fold_results['val_losses'],\n",
        "    'best_val_losses': fold_results['best_val_losses'],\n",
        "    'best_fold_num': best_fold_num,\n",
        "    'best_fold_loss': best_fold_loss,\n",
        "    'avg_best_val_loss': avg_best_val_loss,\n",
        "    'epochs': EPOCHS,\n",
        "    'num_folds': NUM_FOLDS\n",
        "}\n",
        "\n",
        "with open(losses_pickle_path, 'wb') as f:\n",
        "    pickle.dump(losses_data, f)\n",
        "\n",
        "print(f\"\\nTraining and validation losses saved to: {losses_pickle_path}\")\n",
        "print(f\"Data structure:\")\n",
        "print(f\"  - fold_numbers: list of fold IDs\")\n",
        "print(f\"  - train_losses: list of training losses per fold (each fold has {EPOCHS} epochs)\")\n",
        "print(f\"  - val_losses: list of validation losses per fold (each fold has {EPOCHS} epochs)\")\n",
        "print(f\"  - best_val_losses: list of best validation loss for each fold\")\n",
        "print(f\"  - best_fold_num: {best_fold_num}\")\n",
        "print(f\"  - best_fold_loss: {best_fold_loss:.4f}\")\n",
        "print(f\"  - avg_best_val_loss: {avg_best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf5ec91f",
      "metadata": {
        "id": "cf5ec91f"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n best model saved at: {MODEL_SAVE_PATH}, from fold {best_fold_num}, validation loss: {best_fold_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c38879",
      "metadata": {
        "id": "01c38879"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, f1_score, matthews_corrcoef, accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Load best model\n",
        "encoder_eval = BertModel.from_pretrained(MODEL_NAME)\n",
        "encoder_eval.to(DEVICE)\n",
        "model = SPCNNClassifier(encoder_eval, NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "print(f\"\\nModel loaded from {MODEL_SAVE_PATH}\")\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "        # Compute loss using CRF\n",
        "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Decode predictions using CRF\n",
        "        predictions = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Collect valid tokens\n",
        "        for pred_seq, label_seq, mask in zip(predictions, labels, attention_mask):\n",
        "            for pred, true, is_valid in zip(pred_seq, label_seq, mask):\n",
        "                if true.item() != -100 and is_valid.item() == 1:\n",
        "                    all_preds.append(pred)\n",
        "                    all_labels.append(true.item())\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Test Set Results\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=list(label_map.keys())))\n",
        "\n",
        "f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
        "recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
        "mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "token_acc = accuracy_score(all_labels, all_preds)\n",
        "seq_acc = sequence_level_accuracy(all_preds, all_labels, test_label_seqs)\n",
        "avg_loss = test_loss / len(test_loader)\n",
        "\n",
        "print(f\"\\nMetrics Summary:\")\n",
        "print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
        "print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
        "print(f\"Token-level Accuracy: {token_acc:.4f}\")\n",
        "print(f\"Sequence Level Accuracy: {seq_acc:.4f}\")\n",
        "print(f\"Average test loss: {avg_loss:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(label_map.values()))\n",
        "cm_relative = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_relative, display_labels=list(label_map.keys()))\n",
        "disp.plot(cmap=\"OrRd\", xticks_rotation=45)\n",
        "plt.title(f\"Confusion Matrix - Best Model (Fold {best_fold_num})\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
