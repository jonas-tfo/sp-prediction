{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb673bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MODEL_NAME = \"Rostlab/prot_bert\" \n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\" # using mps instead of cuda for training on mac\n",
    "#DEVICE = \"cpu\"  # use GPU if available, otherwise CPU\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "NUM_CLASSES = 6  # num classes for classification\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc30469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define paths to CSV files\n",
    "DATA_PATH = \"/content/drive/MyDrive/PBLRost/data/reduced_30/\"\n",
    "DATA_PATH_FOLDS = \"/content/drive/MyDrive/PBLRost/data/reduced_30/5-fold/\"\n",
    "MODEL_SAVE_PATH = \"/content/drive/MyDrive/PBLRost/models/6state_protbert_lstm_cnn_fold{}.pt\"\n",
    "TEST_CSV = os.path.join(DATA_PATH, \"reduced_30_signalP6_test.csv\")\n",
    "\n",
    "# K-fold cross validation setup\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# Load test data (same for all folds)\n",
    "print(\"Loading test data...\")\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "print(f\"Test records: {len(test_df)}\")\n",
    "\n",
    "# Display sample\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test data\n",
    "print(\"\\nProcessing test data...\")\n",
    "test_df_filtered = test_df[~test_df[\"labels\"].str.contains(\"P\", na=False)]\n",
    "print(f\"Test records after filtering: {len(test_df_filtered)}\")\n",
    "\n",
    "test_df_filtered.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c475a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'S': 0, 'T': 1, 'L': 2, 'I': 3, 'M': 4, 'O': 5}\n",
    "\n",
    "# Process test data\n",
    "test_df_encoded = test_df_filtered.copy()\n",
    "test_df_encoded[\"label\"] = test_df_encoded[\"labels\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
    "test_df_encoded = test_df_encoded[test_df_encoded[\"label\"].map(len) > 0]\n",
    "test_seqs = test_df_encoded[\"sequence\"].tolist()\n",
    "test_label_seqs = test_df_encoded[\"label\"].tolist()\n",
    "\n",
    "print(f\"Test sequences: {len(test_seqs)}\")\n",
    "test_df_encoded.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d54f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
    "encoder.to(DEVICE)\n",
    "print(\"ProtBERT model loaded successfully\")\n",
    "\n",
    "# SPDataset class definition\n",
    "class SPDataset(Dataset):\n",
    "    def __init__(self, sequences, label_seqs, label_map):\n",
    "        self.label_map = label_map\n",
    "        self.label_seqs = label_seqs\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        # preprocess the sequence (insert spaces between amino acids)\n",
    "        seq_processed = \" \".join(list(seq))\n",
    "        labels = self.label_seqs[idx]\n",
    "        # Tokenize the sequence (padding to ensure all sequences are the same length -> 512 tokens) \n",
    "        encoded = tokenizer(seq_processed, return_tensors=\"pt\",\n",
    "                            padding=\"max_length\", truncation=True, max_length=512)\n",
    "        input_ids = encoded['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Build a label tensor of the same length as input_ids.\n",
    "        # For tokens beyond the original sequence length, assign -100 so that loss func ignores them.\n",
    "        orig_length = len(seq)\n",
    "        token_labels = []\n",
    "        \n",
    "        for i in range(input_ids.size(0)):\n",
    "            if i == 0 or i > orig_length:  \n",
    "                token_labels.append(-100)  # ignore padding tokens\n",
    "            else:\n",
    "                # Use the already encoded label directly\n",
    "                token_labels.append(labels[i-1])\n",
    "        labels_tensor = torch.tensor(token_labels)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids, # tokenized and padded \n",
    "            'attention_mask': attention_mask, # differentiate between padding and non-padding tokens\n",
    "            'labels': labels_tensor # aligned label tensor\n",
    "        }\n",
    "\n",
    "# Create test dataset (same for all folds)\n",
    "test_dataset = SPDataset(test_seqs, test_label_seqs, label_map)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nTest data prepared: {len(test_seqs)} sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43de5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchcrf import CRF\n",
    "\n",
    "class SPCNNClassifier(nn.Module):\n",
    "    def __init__(self, encoder_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_model  \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        # detects local features in the sequence\n",
    "        self.conv = nn.Conv1d(in_channels=hidden_size, out_channels=1024, kernel_size=8, dilation=2, padding=7)\n",
    "        # Normalize the convolution output (expects shape: (batch, 1024, seq_len))\n",
    "        self.bn_conv = nn.BatchNorm1d(1024)\n",
    "        # 2 layer long short term memory network\n",
    "        self.lstm = nn.LSTM(input_size=1024, hidden_size=512, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        # dense layer\n",
    "        self.classifier = nn.Linear(512 * 2, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Encode with BERT\n",
    "        encoder_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = encoder_output.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "        #residue_embeddings = output.last_hidden_state[:, 1:-1, :]  # Remove CLS/SEP\n",
    "\n",
    "        #  CNN 1\n",
    "        # Apply conv, then batch normalization and ReLU\n",
    "        x_conv = self.conv(hidden_states.transpose(1, 2))  # (batch, 1024, seq_len)\n",
    "        x_conv = self.bn_conv(x_conv)\n",
    "        x_conv = F.relu_(x_conv)                          # (batch, 1024, seq_len)\n",
    "\n",
    "        # Transpose CNN output for LSTM: (batch, seq_len, features)\n",
    "        x_lstm_input = x_conv.transpose(1, 2)           # (batch, seq_len, 1024)\n",
    "\n",
    "        # Apply BiLSTM\n",
    "        lstm_out, _ = self.lstm(x_lstm_input)            # (batch, seq_len, 1024)\n",
    "\n",
    "        # Classifier to num_labels\n",
    "        x_linear = self.classifier(lstm_out)             # (batch, seq_len, num_labels)\n",
    "        logits = self.dropout(x_linear)                  # (batch, seq_len, num_labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            # Replace ignore-index (-100) with a valid label (0) since CRF doesn't support -100\n",
    "            mod_labels = labels.clone()\n",
    "            mod_labels[labels == -100] = 0\n",
    "            loss = -self.crf(logits, mod_labels, mask=attention_mask.bool(), reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            predictions = self.crf.decode(logits, mask=attention_mask.bool())\n",
    "            return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Function to load and prepare data for a specific fold\n",
    "def prepare_fold_data(fold_num):\n",
    "    \"\"\"Load and prepare data for a specific fold\"\"\"\n",
    "    # Load fold data\n",
    "    train_csv = os.path.join(DATA_PATH, f\"fold_{fold_num}_train.csv\")\n",
    "    val_csv = os.path.join(DATA_PATH, f\"fold_{fold_num}_val.csv\")\n",
    "    \n",
    "    print(f\"\\n=== Fold {fold_num} ===\")\n",
    "    print(f\"Loading training data from: {train_csv}\")\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    print(f\"Training records: {len(train_df)}\")\n",
    "    \n",
    "    print(f\"Loading validation data from: {val_csv}\")\n",
    "    val_df = pd.read_csv(val_csv)\n",
    "    print(f\"Validation records: {len(val_df)}\")\n",
    "    \n",
    "    # Filter data\n",
    "    train_df_filtered = train_df[~train_df[\"labels\"].str.contains(\"P\", na=False)]\n",
    "    val_df_filtered = val_df[~val_df[\"labels\"].str.contains(\"P\", na=False)]\n",
    "    print(f\"Training records after filtering: {len(train_df_filtered)}\")\n",
    "    print(f\"Validation records after filtering: {len(val_df_filtered)}\")\n",
    "    \n",
    "    # Encode labels\n",
    "    train_df_encoded = train_df_filtered.copy()\n",
    "    train_df_encoded[\"label\"] = train_df_encoded[\"labels\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
    "    train_df_encoded = train_df_encoded[train_df_encoded[\"label\"].map(len) > 0]\n",
    "    train_seqs = train_df_encoded[\"sequence\"].tolist()\n",
    "    train_label_seqs = train_df_encoded[\"label\"].tolist()\n",
    "    \n",
    "    val_df_encoded = val_df_filtered.copy()\n",
    "    val_df_encoded[\"label\"] = val_df_encoded[\"labels\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
    "    val_df_encoded = val_df_encoded[val_df_encoded[\"label\"].map(len) > 0]\n",
    "    val_seqs = val_df_encoded[\"sequence\"].tolist()\n",
    "    val_label_seqs = val_df_encoded[\"label\"].tolist()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SPDataset(train_seqs, train_label_seqs, label_map)\n",
    "    val_dataset = SPDataset(val_seqs, val_label_seqs, label_map)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Fold {fold_num} prepared: {len(train_seqs)} train, {len(val_seqs)} val sequences\")\n",
    "    \n",
    "    return train_loader, val_loader, train_seqs, val_seqs\n",
    "\n",
    "print(\"Data preparation function ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d562c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sequence-level accuracy, skipping -100 (ignored) positions\n",
    "def sequence_level_accuracy(preds_flat, labels_flat, test_label_seqs):\n",
    "    # reconstruct the sequences from the flat predictions\n",
    "    seq_lengths = [len(seq) for seq in test_label_seqs]\n",
    "    preds_seq = []\n",
    "    labels_seq = []\n",
    "    idx = 0\n",
    "    for l in seq_lengths:\n",
    "        preds_seq.append(preds_flat[idx:idx+l])\n",
    "        labels_seq.append(labels_flat[idx:idx+l])\n",
    "        idx += l\n",
    "\n",
    "    # check if the valid predictions match the labels\n",
    "    correct = 0\n",
    "    for pred, label in zip(preds_seq, labels_seq):\n",
    "        is_valid = [l != -100 for l in label]\n",
    "        valid_preds = [p for p, valid in zip(pred, is_valid) if valid]\n",
    "        valid_labels = [l for l, valid in zip(label, is_valid) if valid]\n",
    "        if valid_preds == valid_labels:\n",
    "            correct += 1\n",
    "\n",
    "    total = len(seq_lengths)\n",
    "    return correct / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Store results for all folds\n",
    "fold_results = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'best_val_losses': []\n",
    "}\n",
    "\n",
    "# K-Fold Cross Validation Training Loop\n",
    "for fold in range(1, NUM_FOLDS + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting Fold {fold}/{NUM_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Prepare data for this fold\n",
    "    train_loader, val_loader, train_seqs, val_seqs = prepare_fold_data(fold)\n",
    "    \n",
    "    # Initialize fresh model for each fold\n",
    "    encoder_fold = BertModel.from_pretrained(MODEL_NAME)\n",
    "    encoder_fold.to(DEVICE)\n",
    "    model = SPCNNClassifier(encoder_fold, NUM_CLASSES).to(DEVICE)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": model.encoder.encoder.layer[-4:].parameters(), \"lr\": 5e-6},\n",
    "        {\"params\": model.conv.parameters(), \"lr\": 1e-3},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": 1e-3},\n",
    "        {\"params\": model.lstm.parameters(), \"lr\": 1e-3},\n",
    "        {\"params\": model.crf.parameters(), \"lr\": 1e-3},\n",
    "    ])\n",
    "    \n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Freeze encoder parameters initially\n",
    "    for param in model.encoder.encoder.layer[:-10].parameters():\n",
    "        param.requires_grad = False\n",
    "    model.encoder.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Track losses for this fold\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop for this fold\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"Fold {fold} - Epoch {epoch+1}/{EPOCHS} [Train]\", unit=\"batch\")\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch in pbar:\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                token_labels = batch['labels'].to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                loss = model(input_ids, attention_mask, token_labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(\"Error during training:\", e)\n",
    "                gc.collect()\n",
    "                continue\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Fold {fold} - Epoch {epoch+1}/{EPOCHS} [Val]\", unit=\"batch\"):\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                token_labels = batch['labels'].to(DEVICE)\n",
    "                \n",
    "                loss = model(input_ids, attention_mask, token_labels)\n",
    "                total_val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = total_val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Fold {fold} - Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model for this fold\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model_path = MODEL_SAVE_PATH.format(fold)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"  → Best model for fold {fold} saved to {model_path}\")\n",
    "    \n",
    "    # Store results for this fold\n",
    "    fold_results['train_losses'].append(train_losses)\n",
    "    fold_results['val_losses'].append(val_losses)\n",
    "    fold_results['best_val_losses'].append(best_val_loss)\n",
    "    \n",
    "    print(f\"\\nFold {fold} complete! Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Plot losses for this fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, EPOCHS + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {fold} - Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"K-Fold Cross Validation Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print summary of all folds\n",
    "print(\"\\nSummary of all folds:\")\n",
    "for i, best_loss in enumerate(fold_results['best_val_losses'], 1):\n",
    "    print(f\"Fold {i}: Best Validation Loss = {best_loss:.4f}\")\n",
    "\n",
    "avg_best_val_loss = sum(fold_results['best_val_losses']) / NUM_FOLDS\n",
    "print(f\"\\nAverage Best Validation Loss across all folds: {avg_best_val_loss:.4f}\")\n",
    "\n",
    "# Plot comparison across folds\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(NUM_FOLDS):\n",
    "    plt.plot(range(1, EPOCHS + 1), fold_results['val_losses'][i], \n",
    "             label=f'Fold {i+1}', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Across All Folds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ec91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"All fold models saved successfully!\")\n",
    "for i in range(1, NUM_FOLDS + 1):\n",
    "    print(f\"Fold {i}: {MODEL_SAVE_PATH.format(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c38879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on Test Set using all fold models\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# Store predictions from all folds\n",
    "all_fold_predictions = []\n",
    "all_fold_metrics = []\n",
    "\n",
    "print(\"Evaluating all fold models on test set...\\n\")\n",
    "\n",
    "for fold in range(1, NUM_FOLDS + 1):\n",
    "    print(f\"Evaluating Fold {fold} Model\")\n",
    "    \n",
    "    # Load the trained model for this fold\n",
    "    encoder_eval = BertModel.from_pretrained(MODEL_NAME)\n",
    "    encoder_eval.to(DEVICE)\n",
    "    model = SPCNNClassifier(encoder_eval, NUM_CLASSES).to(DEVICE)\n",
    "    \n",
    "    model_path = MODEL_SAVE_PATH.format(fold)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            # Compute loss using CRF\n",
    "            loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Decode predictions using CRF\n",
    "            predictions = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Collect valid tokens\n",
    "            for pred_seq, label_seq, mask in zip(predictions, labels, attention_mask):\n",
    "                for pred, true, is_valid in zip(pred_seq, label_seq, mask):\n",
    "                    if true.item() != -100 and is_valid.item() == 1:\n",
    "                        all_preds.append(pred)\n",
    "                        all_labels.append(true.item())\n",
    "    \n",
    "    # Store predictions for ensemble\n",
    "    all_fold_predictions.append(all_preds)\n",
    "    \n",
    "    # Calculate metrics for this fold\n",
    "    print(f\"\\nFold {fold} Results:\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=list(label_map.keys())))\n",
    "    \n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    token_acc = accuracy_score(all_labels, all_preds)\n",
    "    seq_acc = sequence_level_accuracy(all_preds, all_labels, test_label_seqs)\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "    print(f\"Token-level Accuracy: {token_acc:.4f}\")\n",
    "    print(f\"Sequence Level Accuracy: {seq_acc:.4f}\")\n",
    "    print(f\"Average test loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    all_fold_metrics.append({\n",
    "        'fold': fold,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_macro': f1_macro,\n",
    "        'mcc': mcc,\n",
    "        'token_acc': token_acc,\n",
    "        'seq_acc': seq_acc,\n",
    "        'test_loss': avg_loss\n",
    "    })\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(all_fold_metrics)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nAverage across folds:\")\n",
    "print(f\"F1 (weighted): {metrics_df['f1_weighted'].mean():.4f} ± {metrics_df['f1_weighted'].std():.4f}\")\n",
    "print(f\"F1 (macro): {metrics_df['f1_macro'].mean():.4f} ± {metrics_df['f1_macro'].std():.4f}\")\n",
    "print(f\"MCC: {metrics_df['mcc'].mean():.4f} ± {metrics_df['mcc'].std():.4f}\")\n",
    "print(f\"Token Accuracy: {metrics_df['token_acc'].mean():.4f} ± {metrics_df['token_acc'].std():.4f}\")\n",
    "print(f\"Sequence Accuracy: {metrics_df['seq_acc'].mean():.4f} ± {metrics_df['seq_acc'].std():.4f}\")\n",
    "print(f\"Test Loss: {metrics_df['test_loss'].mean():.4f} ± {metrics_df['test_loss'].std():.4f}\")\n",
    "\n",
    "# Confusion Matrix for best performing fold\n",
    "best_fold_idx = metrics_df['f1_weighted'].idxmax()\n",
    "best_fold_num = all_fold_metrics[best_fold_idx]['fold']\n",
    "\n",
    "print(f\"\\nShowing confusion matrix for best performing fold: Fold {best_fold_num}\")\n",
    "\n",
    "# Re-evaluate best fold for confusion matrix\n",
    "encoder_best = BertModel.from_pretrained(MODEL_NAME)\n",
    "encoder_best.to(DEVICE)\n",
    "model_best = SPCNNClassifier(encoder_best, NUM_CLASSES).to(DEVICE)\n",
    "model_best.load_state_dict(torch.load(MODEL_SAVE_PATH.format(best_fold_num), map_location=DEVICE))\n",
    "model_best.eval()\n",
    "\n",
    "all_preds_best = []\n",
    "all_labels_best = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        predictions = model_best(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        for pred_seq, label_seq, mask in zip(predictions, labels, attention_mask):\n",
    "            for pred, true, is_valid in zip(pred_seq, label_seq, mask):\n",
    "                if true.item() != -100 and is_valid.item() == 1:\n",
    "                    all_preds_best.append(pred)\n",
    "                    all_labels_best.append(true.item())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(all_labels_best, all_preds_best, labels=list(label_map.values()))\n",
    "cm_relative = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_relative, display_labels=list(label_map.keys()))\n",
    "disp.plot(cmap=\"OrRd\", xticks_rotation=45)\n",
    "plt.title(f\"Confusion Matrix - Fold {best_fold_num} (Best)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75761736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def draw_vertical_model():\n",
    "    fig, ax = plt.subplots(figsize=(6, 10))\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Define blocks with (x, y)\n",
    "    blocks = [\n",
    "        (\"Input IDs\", 0.5, 9.0),\n",
    "        (\"Attention Mask\", 2.5, 9.0),\n",
    "        (\"Encoder\\n(BERT-like)\", 1.5, 8.0),\n",
    "        (\"Conv1D\\n(1 layer, kernel=8, dilation=2)\", 1.5, 7.0),\n",
    "        (\"BatchNorm1D + ReLU\", 1.5, 6.0),\n",
    "        (\"BiLSTM\\n(2 layers, hidden=512)\", 1.5, 5.0),\n",
    "        (\"Dense\\n(1024 → num_label (6))\", 1.5, 4.0),\n",
    "        (\"Dropout\\n(p=0.2)\", 1.5, 3.0),\n",
    "        (\"CRF Layer\", 1.5, 2.0),\n",
    "        (\"Per token Predictions\", 1.5, 1.0)\n",
    "    ]\n",
    "\n",
    "    box_width = 2.0\n",
    "    box_height = 0.6\n",
    "\n",
    "    # Draw blocks\n",
    "    for label, x, y in blocks:\n",
    "        rect = mpatches.FancyBboxPatch((x, y), box_width, box_height, boxstyle=\"round,pad=0.03\",\n",
    "                                       edgecolor='black', facecolor='skyblue', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + box_width / 2, y + box_height / 2, label, ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Draw arrows\n",
    "    for i in range(2, len(blocks) - 1):  # skip input IDs and mask arrows\n",
    "        x1 = blocks[i][1] + box_width / 2\n",
    "        y1 = blocks[i][2]\n",
    "        y2 = blocks[i+1][2] + box_height\n",
    "        ax.annotate('', xy=(x1, y2), xytext=(x1, y1),\n",
    "                    arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "\n",
    "    # Draw arrows from inputs\n",
    "    ax.annotate('', xy=(1.5 + box_width/2, 8.6), xytext=(0.5 + box_width/2, 9.0),\n",
    "                arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "    ax.annotate('', xy=(1.5 + box_width/2, 8.6), xytext=(2.5 + box_width/2, 9.0),\n",
    "                arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "\n",
    "    plt.title(\"SPCNNClassifier Architecture\", fontsize=14)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.xlim(0, 5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "draw_vertical_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
