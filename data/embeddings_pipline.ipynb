{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ef581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Project base directory set to: /home/jonas/Documents/Projects/sp-prediction\n",
      "Data path set to: /home/jonas/Documents/Projects/sp-prediction/data/aufgabe3\n",
      "Model save path set to: /home/jonas/Documents/Projects/sp-prediction/models/6state_t5_lstm_cnn/6state_t5_lstm_cnn.pt\n",
      "Using model: Rostlab/prot_t5_xl_half_uniref50-enc\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from pathlib import Path\n",
    "\n",
    "# \"Rostlab/prot_t5_xl_half_uniref50-enc\" (~1.2B params)\n",
    "# \"Rostlab/prot_t5_base_mt_uniref50\" (~220M params) \n",
    "MODEL_NAME = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "NUM_FOLDS = 3\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "BASE_DIR = notebook_dir.parent\n",
    "\n",
    "DATA_PATH = BASE_DIR / \"data\" / \"aufgabe3\"\n",
    "DATA_PATH_FOLDS = DATA_PATH / \"3-fold\"\n",
    "MODEL_SAVE_PATH_TEMP = str(BASE_DIR / \"models\" / \"6state_t5_lstm_cnn_fold{}.pt\")\n",
    "MODEL_SAVE_PATH = BASE_DIR / \"models\" / \"6state_t5_lstm_cnn\" / \"6state_t5_lstm_cnn.pt\"\n",
    "TRAIN_VAL_LOSSES_DATA_SAVE_PATH = DATA_PATH / \"6state_t5_lstm_cnn\" / \"outputs\"\n",
    "TEST_CSV = DATA_PATH / \"reduced_30_signalP6_test.csv\"\n",
    "\n",
    "(BASE_DIR / \"models\" / \"6state_t5_lstm_cnn\").mkdir(parents=True, exist_ok=True)\n",
    "(DATA_PATH / \"6state_t5_lstm_cnn\" / \"outputs\").mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Project base directory set to: {BASE_DIR}\")\n",
    "print(f\"Data path set to: {DATA_PATH}\")\n",
    "print(f\"Model save path set to: {MODEL_SAVE_PATH}\")\n",
    "print(f\"Using model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57891fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sequence(sequence: str, tokenizer, encoder, device: str = DEVICE, pooling: str = \"mean\") -> torch.Tensor:\n",
    "\n",
    "    seq_spaced = \" \".join(list(sequence))\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(seq_spaced, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = tokenized['input_ids'].to(device)\n",
    "    attention_mask = tokenized['attention_mask'].to(device)\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        output = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = output.last_hidden_state  # (1, seq_len, hidden_dim)\n",
    "    \n",
    "    # Remove batch dimension\n",
    "    embeddings = embeddings.squeeze(0)  # (seq_len, hidden_dim)\n",
    "    \n",
    "    # Get sequence length (excluding EOS token)\n",
    "    seq_len = len(sequence)\n",
    "    embeddings = embeddings[:seq_len]  # (seq_len, hidden_dim)\n",
    "    \n",
    "    if pooling == \"mean\":\n",
    "        return embeddings.mean(dim=0).float()  # (hidden_dim,)\n",
    "    elif pooling == \"none\":\n",
    "        return embeddings.float()  # (seq_len, hidden_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pooling method: {pooling}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffb0dd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5EncoderModel(\n",
       "  (shared): Embedding(128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "encoder = T5EncoderModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16)\n",
    "encoder.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc5bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding sequences for fold 1 train set:\n",
      "  Processed 100/9116 sequences\n",
      "  Processed 200/9116 sequences\n",
      "  Processed 300/9116 sequences\n",
      "  Processed 400/9116 sequences\n",
      "  Processed 500/9116 sequences\n",
      "  Processed 600/9116 sequences\n",
      "  Processed 700/9116 sequences\n",
      "  Processed 800/9116 sequences\n",
      "  Processed 900/9116 sequences\n",
      "  Processed 1000/9116 sequences\n",
      "  Processed 1100/9116 sequences\n",
      "  Processed 1200/9116 sequences\n",
      "  Processed 1300/9116 sequences\n",
      "  Processed 1400/9116 sequences\n",
      "  Processed 1500/9116 sequences\n",
      "  Processed 1600/9116 sequences\n",
      "  Processed 1700/9116 sequences\n",
      "  Processed 1800/9116 sequences\n",
      "  Processed 1900/9116 sequences\n",
      "  Processed 2000/9116 sequences\n",
      "  Processed 2100/9116 sequences\n",
      "  Processed 2200/9116 sequences\n",
      "  Processed 2300/9116 sequences\n",
      "  Processed 2400/9116 sequences\n",
      "  Processed 2500/9116 sequences\n",
      "  Processed 2600/9116 sequences\n",
      "  Processed 2700/9116 sequences\n",
      "  Processed 2800/9116 sequences\n",
      "  Processed 2900/9116 sequences\n",
      "  Processed 3000/9116 sequences\n",
      "  Processed 3100/9116 sequences\n",
      "  Processed 3200/9116 sequences\n",
      "  Processed 3300/9116 sequences\n",
      "  Processed 3400/9116 sequences\n",
      "  Processed 3500/9116 sequences\n",
      "  Processed 3600/9116 sequences\n",
      "  Processed 3700/9116 sequences\n",
      "  Processed 3800/9116 sequences\n",
      "  Processed 3900/9116 sequences\n",
      "  Processed 4000/9116 sequences\n",
      "  Processed 4100/9116 sequences\n",
      "  Processed 4200/9116 sequences\n",
      "  Processed 4300/9116 sequences\n",
      "  Processed 4400/9116 sequences\n",
      "  Processed 4500/9116 sequences\n",
      "  Processed 4600/9116 sequences\n",
      "  Processed 4700/9116 sequences\n",
      "  Processed 4800/9116 sequences\n",
      "  Processed 4900/9116 sequences\n",
      "  Processed 5000/9116 sequences\n",
      "  Processed 5100/9116 sequences\n",
      "  Processed 5200/9116 sequences\n",
      "  Processed 5300/9116 sequences\n",
      "  Processed 5400/9116 sequences\n",
      "  Processed 5500/9116 sequences\n",
      "  Processed 5600/9116 sequences\n",
      "  Processed 5700/9116 sequences\n",
      "  Processed 5800/9116 sequences\n",
      "  Processed 5900/9116 sequences\n",
      "  Processed 6000/9116 sequences\n",
      "  Processed 6100/9116 sequences\n",
      "  Processed 6200/9116 sequences\n",
      "  Processed 6300/9116 sequences\n",
      "  Processed 6400/9116 sequences\n",
      "  Processed 6500/9116 sequences\n",
      "  Processed 6600/9116 sequences\n",
      "  Processed 6700/9116 sequences\n",
      "  Processed 6800/9116 sequences\n",
      "  Processed 6900/9116 sequences\n",
      "  Processed 7000/9116 sequences\n",
      "  Processed 7100/9116 sequences\n",
      "  Processed 7200/9116 sequences\n",
      "  Processed 7300/9116 sequences\n",
      "  Processed 7400/9116 sequences\n",
      "  Processed 7500/9116 sequences\n",
      "  Processed 7600/9116 sequences\n",
      "  Processed 7700/9116 sequences\n",
      "  Processed 7800/9116 sequences\n",
      "  Processed 7900/9116 sequences\n",
      "  Processed 8000/9116 sequences\n",
      "  Processed 8100/9116 sequences\n",
      "  Processed 8200/9116 sequences\n",
      "  Processed 8300/9116 sequences\n",
      "  Processed 8400/9116 sequences\n",
      "  Processed 8500/9116 sequences\n",
      "  Processed 8600/9116 sequences\n",
      "  Processed 8700/9116 sequences\n",
      "  Processed 8800/9116 sequences\n",
      "  Processed 8900/9116 sequences\n",
      "  Processed 9000/9116 sequences\n",
      "  Processed 9100/9116 sequences\n",
      "  Saved 9116 embeddings to fold_1_train_embeddings.npz\n",
      "Embedding sequences for fold 1 val set:\n",
      "  Processed 100/4559 sequences\n",
      "  Processed 200/4559 sequences\n",
      "  Processed 300/4559 sequences\n",
      "  Processed 400/4559 sequences\n",
      "  Processed 500/4559 sequences\n",
      "  Processed 600/4559 sequences\n",
      "  Processed 700/4559 sequences\n",
      "  Processed 800/4559 sequences\n",
      "  Processed 900/4559 sequences\n",
      "  Processed 1000/4559 sequences\n",
      "  Processed 1100/4559 sequences\n",
      "  Processed 1200/4559 sequences\n",
      "  Processed 1300/4559 sequences\n",
      "  Processed 1400/4559 sequences\n",
      "  Processed 1500/4559 sequences\n",
      "  Processed 1600/4559 sequences\n",
      "  Processed 1700/4559 sequences\n",
      "  Processed 1800/4559 sequences\n",
      "  Processed 1900/4559 sequences\n",
      "  Processed 2000/4559 sequences\n",
      "  Processed 2100/4559 sequences\n",
      "  Processed 2200/4559 sequences\n",
      "  Processed 2300/4559 sequences\n",
      "  Processed 2400/4559 sequences\n",
      "  Processed 2500/4559 sequences\n",
      "  Processed 2600/4559 sequences\n",
      "  Processed 2700/4559 sequences\n",
      "  Processed 2800/4559 sequences\n",
      "  Processed 2900/4559 sequences\n",
      "  Processed 3000/4559 sequences\n",
      "  Processed 3100/4559 sequences\n",
      "  Processed 3200/4559 sequences\n",
      "  Processed 3300/4559 sequences\n",
      "  Processed 3400/4559 sequences\n",
      "  Processed 3500/4559 sequences\n",
      "  Processed 3600/4559 sequences\n",
      "  Processed 3700/4559 sequences\n",
      "  Processed 3800/4559 sequences\n",
      "  Processed 3900/4559 sequences\n",
      "  Processed 4000/4559 sequences\n",
      "  Processed 4100/4559 sequences\n",
      "  Processed 4200/4559 sequences\n",
      "  Processed 4300/4559 sequences\n",
      "  Processed 4400/4559 sequences\n",
      "  Processed 4500/4559 sequences\n",
      "  Saved 4559 embeddings to fold_1_val_embeddings.npz\n",
      "Embedding sequences for fold 2 train set:\n",
      "  Processed 100/9117 sequences\n",
      "  Processed 200/9117 sequences\n",
      "  Processed 300/9117 sequences\n",
      "  Processed 400/9117 sequences\n",
      "  Processed 500/9117 sequences\n",
      "  Processed 600/9117 sequences\n",
      "  Processed 700/9117 sequences\n",
      "  Processed 800/9117 sequences\n",
      "  Processed 900/9117 sequences\n",
      "  Processed 1000/9117 sequences\n",
      "  Processed 1100/9117 sequences\n",
      "  Processed 1200/9117 sequences\n",
      "  Processed 1300/9117 sequences\n",
      "  Processed 1400/9117 sequences\n",
      "  Processed 1500/9117 sequences\n",
      "  Processed 1600/9117 sequences\n",
      "  Processed 1700/9117 sequences\n",
      "  Processed 1800/9117 sequences\n",
      "  Processed 1900/9117 sequences\n",
      "  Processed 2000/9117 sequences\n",
      "  Processed 2100/9117 sequences\n",
      "  Processed 2200/9117 sequences\n",
      "  Processed 2300/9117 sequences\n",
      "  Processed 2400/9117 sequences\n",
      "  Processed 2500/9117 sequences\n",
      "  Processed 2600/9117 sequences\n",
      "  Processed 2700/9117 sequences\n",
      "  Processed 2800/9117 sequences\n",
      "  Processed 2900/9117 sequences\n",
      "  Processed 3000/9117 sequences\n",
      "  Processed 3100/9117 sequences\n",
      "  Processed 3200/9117 sequences\n",
      "  Processed 3300/9117 sequences\n",
      "  Processed 3400/9117 sequences\n",
      "  Processed 3500/9117 sequences\n",
      "  Processed 3600/9117 sequences\n",
      "  Processed 3700/9117 sequences\n",
      "  Processed 3800/9117 sequences\n",
      "  Processed 3900/9117 sequences\n",
      "  Processed 4000/9117 sequences\n",
      "  Processed 4100/9117 sequences\n",
      "  Processed 4200/9117 sequences\n",
      "  Processed 4300/9117 sequences\n",
      "  Processed 4400/9117 sequences\n",
      "  Processed 4500/9117 sequences\n",
      "  Processed 4600/9117 sequences\n",
      "  Processed 4700/9117 sequences\n",
      "  Processed 4800/9117 sequences\n",
      "  Processed 4900/9117 sequences\n",
      "  Processed 5000/9117 sequences\n",
      "  Processed 5100/9117 sequences\n",
      "  Processed 5200/9117 sequences\n",
      "  Processed 5300/9117 sequences\n",
      "  Processed 5400/9117 sequences\n",
      "  Processed 5500/9117 sequences\n",
      "  Processed 5600/9117 sequences\n",
      "  Processed 5700/9117 sequences\n",
      "  Processed 5800/9117 sequences\n",
      "  Processed 5900/9117 sequences\n",
      "  Processed 6000/9117 sequences\n",
      "  Processed 6100/9117 sequences\n",
      "  Processed 6200/9117 sequences\n",
      "  Processed 6300/9117 sequences\n",
      "  Processed 6400/9117 sequences\n",
      "  Processed 6500/9117 sequences\n",
      "  Processed 6600/9117 sequences\n",
      "  Processed 6700/9117 sequences\n",
      "  Processed 6800/9117 sequences\n",
      "  Processed 6900/9117 sequences\n",
      "  Processed 7000/9117 sequences\n",
      "  Processed 7100/9117 sequences\n",
      "  Processed 7200/9117 sequences\n",
      "  Processed 7300/9117 sequences\n",
      "  Processed 7400/9117 sequences\n",
      "  Processed 7500/9117 sequences\n",
      "  Processed 7600/9117 sequences\n",
      "  Processed 7700/9117 sequences\n",
      "  Processed 7800/9117 sequences\n",
      "  Processed 7900/9117 sequences\n",
      "  Processed 8000/9117 sequences\n",
      "  Processed 8100/9117 sequences\n",
      "  Processed 8200/9117 sequences\n",
      "  Processed 8300/9117 sequences\n",
      "  Processed 8400/9117 sequences\n",
      "  Processed 8500/9117 sequences\n",
      "  Processed 8600/9117 sequences\n",
      "  Processed 8700/9117 sequences\n",
      "  Processed 8800/9117 sequences\n",
      "  Processed 8900/9117 sequences\n",
      "  Processed 9000/9117 sequences\n",
      "  Processed 9100/9117 sequences\n",
      "  Saved 9117 embeddings to fold_2_train_embeddings.npz\n",
      "Embedding sequences for fold 2 val set:\n",
      "  Processed 100/4558 sequences\n",
      "  Processed 200/4558 sequences\n",
      "  Processed 300/4558 sequences\n",
      "  Processed 400/4558 sequences\n",
      "  Processed 500/4558 sequences\n",
      "  Processed 600/4558 sequences\n",
      "  Processed 700/4558 sequences\n",
      "  Processed 800/4558 sequences\n",
      "  Processed 900/4558 sequences\n",
      "  Processed 1000/4558 sequences\n",
      "  Processed 1100/4558 sequences\n",
      "  Processed 1200/4558 sequences\n",
      "  Processed 1300/4558 sequences\n",
      "  Processed 1400/4558 sequences\n",
      "  Processed 1500/4558 sequences\n",
      "  Processed 1600/4558 sequences\n",
      "  Processed 1700/4558 sequences\n",
      "  Processed 1800/4558 sequences\n",
      "  Processed 1900/4558 sequences\n",
      "  Processed 2000/4558 sequences\n",
      "  Processed 2100/4558 sequences\n",
      "  Processed 2200/4558 sequences\n",
      "  Processed 2300/4558 sequences\n",
      "  Processed 2400/4558 sequences\n",
      "  Processed 2500/4558 sequences\n",
      "  Processed 2600/4558 sequences\n",
      "  Processed 2700/4558 sequences\n",
      "  Processed 2800/4558 sequences\n",
      "  Processed 2900/4558 sequences\n",
      "  Processed 3000/4558 sequences\n",
      "  Processed 3100/4558 sequences\n",
      "  Processed 3200/4558 sequences\n",
      "  Processed 3300/4558 sequences\n",
      "  Processed 3400/4558 sequences\n",
      "  Processed 3500/4558 sequences\n",
      "  Processed 3600/4558 sequences\n",
      "  Processed 3700/4558 sequences\n",
      "  Processed 3800/4558 sequences\n",
      "  Processed 3900/4558 sequences\n",
      "  Processed 4000/4558 sequences\n",
      "  Processed 4100/4558 sequences\n",
      "  Processed 4200/4558 sequences\n",
      "  Processed 4300/4558 sequences\n",
      "  Processed 4400/4558 sequences\n",
      "  Processed 4500/4558 sequences\n",
      "  Saved 4558 embeddings to fold_2_val_embeddings.npz\n",
      "Embedding sequences for fold 3 train set:\n",
      "  Processed 100/9117 sequences\n",
      "  Processed 200/9117 sequences\n",
      "  Processed 300/9117 sequences\n",
      "  Processed 400/9117 sequences\n",
      "  Processed 500/9117 sequences\n",
      "  Processed 600/9117 sequences\n",
      "  Processed 700/9117 sequences\n",
      "  Processed 800/9117 sequences\n",
      "  Processed 900/9117 sequences\n",
      "  Processed 1000/9117 sequences\n",
      "  Processed 1100/9117 sequences\n",
      "  Processed 1200/9117 sequences\n",
      "  Processed 1300/9117 sequences\n",
      "  Processed 1400/9117 sequences\n",
      "  Processed 1500/9117 sequences\n",
      "  Processed 1600/9117 sequences\n",
      "  Processed 1700/9117 sequences\n",
      "  Processed 1800/9117 sequences\n",
      "  Processed 1900/9117 sequences\n",
      "  Processed 2000/9117 sequences\n",
      "  Processed 2100/9117 sequences\n",
      "  Processed 2200/9117 sequences\n",
      "  Processed 2300/9117 sequences\n",
      "  Processed 2400/9117 sequences\n",
      "  Processed 2500/9117 sequences\n",
      "  Processed 2600/9117 sequences\n",
      "  Processed 2700/9117 sequences\n",
      "  Processed 2800/9117 sequences\n",
      "  Processed 2900/9117 sequences\n",
      "  Processed 3000/9117 sequences\n",
      "  Processed 3100/9117 sequences\n",
      "  Processed 3200/9117 sequences\n",
      "  Processed 3300/9117 sequences\n",
      "  Processed 3400/9117 sequences\n",
      "  Processed 3500/9117 sequences\n",
      "  Processed 3600/9117 sequences\n",
      "  Processed 3700/9117 sequences\n",
      "  Processed 3800/9117 sequences\n",
      "  Processed 3900/9117 sequences\n",
      "  Processed 4000/9117 sequences\n",
      "  Processed 4100/9117 sequences\n",
      "  Processed 4200/9117 sequences\n",
      "  Processed 4300/9117 sequences\n",
      "  Processed 4400/9117 sequences\n",
      "  Processed 4500/9117 sequences\n",
      "  Processed 4600/9117 sequences\n",
      "  Processed 4700/9117 sequences\n",
      "  Processed 4800/9117 sequences\n",
      "  Processed 4900/9117 sequences\n",
      "  Processed 5000/9117 sequences\n",
      "  Processed 5100/9117 sequences\n",
      "  Processed 5200/9117 sequences\n",
      "  Processed 5300/9117 sequences\n",
      "  Processed 5400/9117 sequences\n",
      "  Processed 5500/9117 sequences\n",
      "  Processed 5600/9117 sequences\n",
      "  Processed 5700/9117 sequences\n",
      "  Processed 5800/9117 sequences\n",
      "  Processed 5900/9117 sequences\n",
      "  Processed 6000/9117 sequences\n",
      "  Processed 6100/9117 sequences\n",
      "  Processed 6200/9117 sequences\n",
      "  Processed 6300/9117 sequences\n",
      "  Processed 6400/9117 sequences\n",
      "  Processed 6500/9117 sequences\n",
      "  Processed 6600/9117 sequences\n",
      "  Processed 6700/9117 sequences\n",
      "  Processed 6800/9117 sequences\n",
      "  Processed 6900/9117 sequences\n",
      "  Processed 7000/9117 sequences\n",
      "  Processed 7100/9117 sequences\n",
      "  Processed 7200/9117 sequences\n",
      "  Processed 7300/9117 sequences\n",
      "  Processed 7400/9117 sequences\n",
      "  Processed 7500/9117 sequences\n",
      "  Processed 7600/9117 sequences\n",
      "  Processed 7700/9117 sequences\n",
      "  Processed 7800/9117 sequences\n",
      "  Processed 7900/9117 sequences\n",
      "  Processed 8000/9117 sequences\n",
      "  Processed 8100/9117 sequences\n",
      "  Processed 8200/9117 sequences\n",
      "  Processed 8300/9117 sequences\n",
      "  Processed 8400/9117 sequences\n",
      "  Processed 8500/9117 sequences\n",
      "  Processed 8600/9117 sequences\n",
      "  Processed 8700/9117 sequences\n",
      "  Processed 8800/9117 sequences\n",
      "  Processed 8900/9117 sequences\n",
      "  Processed 9000/9117 sequences\n",
      "  Processed 9100/9117 sequences\n",
      "  Saved 9117 embeddings to fold_3_train_embeddings.npz\n",
      "Embedding sequences for fold 3 val set:\n",
      "  Processed 100/4558 sequences\n",
      "  Processed 200/4558 sequences\n",
      "  Processed 300/4558 sequences\n",
      "  Processed 400/4558 sequences\n",
      "  Processed 500/4558 sequences\n",
      "  Processed 600/4558 sequences\n",
      "  Processed 700/4558 sequences\n",
      "  Processed 800/4558 sequences\n",
      "  Processed 900/4558 sequences\n",
      "  Processed 1000/4558 sequences\n",
      "  Processed 1100/4558 sequences\n",
      "  Processed 1200/4558 sequences\n",
      "  Processed 1300/4558 sequences\n",
      "  Processed 1400/4558 sequences\n",
      "  Processed 1500/4558 sequences\n",
      "  Processed 1600/4558 sequences\n",
      "  Processed 1700/4558 sequences\n",
      "  Processed 1800/4558 sequences\n",
      "  Processed 1900/4558 sequences\n",
      "  Processed 2000/4558 sequences\n",
      "  Processed 2100/4558 sequences\n",
      "  Processed 2200/4558 sequences\n",
      "  Processed 2300/4558 sequences\n",
      "  Processed 2400/4558 sequences\n",
      "  Processed 2500/4558 sequences\n",
      "  Processed 2600/4558 sequences\n",
      "  Processed 2700/4558 sequences\n",
      "  Processed 2800/4558 sequences\n",
      "  Processed 2900/4558 sequences\n",
      "  Processed 3000/4558 sequences\n",
      "  Processed 3100/4558 sequences\n",
      "  Processed 3200/4558 sequences\n",
      "  Processed 3300/4558 sequences\n",
      "  Processed 3400/4558 sequences\n",
      "  Processed 3500/4558 sequences\n",
      "  Processed 3600/4558 sequences\n",
      "  Processed 3700/4558 sequences\n",
      "  Processed 3800/4558 sequences\n",
      "  Processed 3900/4558 sequences\n",
      "  Processed 4000/4558 sequences\n",
      "  Processed 4100/4558 sequences\n",
      "  Processed 4200/4558 sequences\n",
      "  Processed 4300/4558 sequences\n",
      "  Processed 4400/4558 sequences\n",
      "  Processed 4500/4558 sequences\n",
      "  Saved 4558 embeddings to fold_3_val_embeddings.npz\n",
      "\n",
      "All embeddings generated and saved!\n",
      "Embedding keys: ['fold_1_train', 'fold_1_val', 'fold_2_train', 'fold_2_val', 'fold_3_train', 'fold_3_val']\n"
     ]
    }
   ],
   "source": [
    "# Generate and save embeddings for each fold's train/val sets\n",
    "all_embeddings = {} # will contain as a value a dict of \"uniprot_id: embedding\" for all sequences in that fold split\n",
    "\n",
    "for fold_idx in range(NUM_FOLDS):\n",
    "\n",
    "    # load data for according fold\n",
    "    fold_train = pd.read_csv(DATA_PATH_FOLDS / f\"fold_{fold_idx + 1}_train.csv\")\n",
    "    fold_val = pd.read_csv(DATA_PATH_FOLDS / f\"fold_{fold_idx + 1}_val.csv\")\n",
    "    \n",
    "    # iterate over train and val sets\n",
    "    for df, split in [(fold_train, \"train\"), (fold_val, \"val\")]:\n",
    "        embeddings_dict = {}\n",
    "        print(f\"Embedding sequences for fold {fold_idx + 1} {split} set:\")\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            uniprot_id = row['uniprot_id']\n",
    "            sequence = row['sequence']\n",
    "            embedding = embed_sequence(sequence, tokenizer, encoder, pooling=\"none\") # embed per residue\n",
    "            embeddings_dict[uniprot_id] = embedding.cpu().numpy() # save to dict as numpy array for later use for npz saving\n",
    "            \n",
    "            if (len(embeddings_dict)) % 100 == 0:\n",
    "                print(f\"  Processed {len(embeddings_dict)}/{len(df)} sequences\")\n",
    "        \n",
    "        # store in main dict with local dict as value    \n",
    "        key = f\"fold_{fold_idx + 1}_{split}\"\n",
    "        all_embeddings[key] = embeddings_dict\n",
    "        \n",
    "        # save embeddings to npz file\n",
    "        save_path = DATA_PATH_FOLDS / f\"fold_{fold_idx + 1}_{split}_embeddings.npz\"\n",
    "        np.savez(save_path, **embeddings_dict)\n",
    "        print(f\"  Saved {len(embeddings_dict)} embeddings to {save_path.name}\")\n",
    "\n",
    "print(f\"\\nAll embeddings generated and saved!\")\n",
    "print(f\"Embedding keys: {list(all_embeddings.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1df887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b1793",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/sp-prediction/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m sample_id = train_df.iloc[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33muniprot_id\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m sample_embedding = embeddings_data[sample_id]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m sample_label = \u001b[43mtrain_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUniprot ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_embedding.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/sp-prediction/.venv/lib/python3.12/site-packages/pandas/core/series.py:1133\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/sp-prediction/.venv/lib/python3.12/site-packages/pandas/core/series.py:1249\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1248\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/sp-prediction/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "# Example: Loading embeddings during training\n",
    "# ==========================================\n",
    "\n",
    "# Load embeddings file\n",
    "embeddings_data = np.load(DATA_PATH_FOLDS / \"fold_1_train_embeddings.npz\")\n",
    "\n",
    "# Load corresponding CSV with labels\n",
    "train_df = pd.read_csv(DATA_PATH_FOLDS / \"fold_1_train.csv\")\n",
    "\n",
    "# Access embedding by uniprot_id\n",
    "sample_id = train_df.iloc[0]['uniprot_id']\n",
    "sample_embedding = embeddings_data[sample_id]\n",
    "sample_label = train_df.iloc[0]['labels']\n",
    "\n",
    "print(f\"Uniprot ID: {sample_id}\")\n",
    "print(f\"Embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"Label: {sample_label}\")\n",
    "\n",
    "# For training, iterate over dataframe and fetch embeddings:\n",
    "# for idx, row in train_df.iterrows():\n",
    "#     embedding = embeddings_data[row['uniprot_id']]\n",
    "#     label = row['label']\n",
    "#     # ... use in training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
